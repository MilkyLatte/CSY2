{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMS21202: Symbols, Patterns and Signals #\n",
    "# Lab 4: Probabilistic Models #\n",
    "\n",
    "---\n",
    "Tip: You will need to refer to the lecture slides and the help pages to complete this exercise.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from skimage import data, io, color, transform, exposure\n",
    "from pprint import pprint\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline\n",
    "# notebook\n",
    "import matplotlib.pylab as pylab\n",
    "pylab.rcParams['figure.figsize'] = (32.0, 24.0)\n",
    "pylab.rcParams['font.size'] = 24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Normal distribution ##\n",
    "You believe that your data follows a normal distribution. Assuming the standard deviation ($\\sigma$) is 0.5, you wish to estimate the mean $\\mu$ of the normal distribution representing your data.\n",
    "You thus have a model with a single parameter $\\mu$ you wish to tune/train.\n",
    "Discuss with your lab partner how the likelihood $p(D|\\mu)$ for some observations $D = \\{d_1, \\cdots, d_N\\}$ can be represented by (assuming independent observations):\n",
    "\\begin{align}\n",
    "p(D|\\theta) &= \\prod_i \\mathcal{N}(d_i|\\theta, 0.25)\\\\\n",
    "&= \\prod_i \\frac{2}{\\sqrt{2 \\pi}} e^{-2(d_i - \\mu)^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MLE ##\n",
    "Use the Maximum Likelihood Estimate (MLE) recipe to find $\\mu_{ML}$.  \n",
    "*Note:* This is done on paper (or preferably in $\\LaTeX$ in the cell below), not using `Python`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE Derivation ###\n",
    "\n",
    "Put your latex derivation here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. MLE - Experimentally ##\n",
    "In this lab, we aim to help you understand MLE by experimenting with different values of $\\mu$ to find $\\mu_{ML} = arg\\,max_\\mu \\, p(D|\\mu)$.\n",
    "\n",
    "**a) **Load the data from file `data1.dat` and plot a histogram of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) ** Write a function `computeLikelihood(D, mu)` that takes a value of $\\mu$ (e.g. $\\mu$ = 0), and computes $p(D | \\mu)$ using equation given in *Q1* for the data in `data1.dat`.  You may use the `Python` functions `stats.norm.pdf` and `np.prod` in the calculation. Do not use a loop\n",
    "\n",
    "$$\n",
    "p(D | \\mu) = \\prod_i \\frac{2}{\\sqrt{2 \\pi}} e^{-2(d_i - \\mu)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) **Write a function `loopLikelihood(D)` that loops through possible values of  \n",
    "$\\mu \\in \\{0.00, 0.01, 0.02, ... , 1.00\\}$, calls `computeLikelihood(D, mu)` for each value and stores an array of all likelihood values.\n",
    "\n",
    "You may use a loop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) **Based on your calculation, what would $\\max p(D|\\mu)$ be? What would $arg\\,max_\\mu \\, p(D|\\mu)$ be? Make sure you understand the difference between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) **Plot $\\mu$ against $p(D|\\mu)$ for different $\\mu$ values. Can you visually spot $\\mu_{ML}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f) **How does this compare to $\\mu_{ML}$ you concluded in *Q2* of this sheet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**g) **Assume you have prior knowledge of what $\\mu_{ML}$ should be, $p(\\mu) = \\mathcal{N}(0.5,0.01)$. Write functions `computePosterior(D, mu)` and `loopPosterior(D)` to find  \n",
    "$\\mu_{MAP} = \\arg \\max_{\\mu} p(D|\\mu)p(\\mu)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h) **plot $\\mu$ against both $p(D|\\mu)$ and $p(D|\\mu)p(\\mu)$ similar to the graph below.\n",
    "![MLE](mle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i) **Repeat the above calculations for `data2.dat` and `data3.dat` and explain your observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTRA 1: **Until now, you assumed $\\sigma$ = 0.5. Remove this assumption and estimate $\\theta_{MAP} = [\\mu_{MAP}, \\sigma_{MAP}]$ experimentally by looping through possible values of $\\mu$ and $\\sigma$. Assume the prior probability for $p(\\sigma)$ is $\\mathcal{N}(0.5, 0.16)$.\n",
    "\n",
    "---\n",
    "Tip: You may need to use `np.nanargmax` instead of `np.argmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTRA 2: **Plot ($\\mu$, $\\sigma$) against $p(D|\\theta)p(\\theta)$ similar to the mesh graph below (use the function `Axes3D.plot_surface` in `Python`).\n",
    "![MLE mesh](mle2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
